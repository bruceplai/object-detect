{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from yolov4.tf import YOLOv4\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Uncomment if using CPU to do training and inference\n",
    "# tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = os.path.join('classes', 'coco.names')\n",
    "weight_file = os.path.join('weights', 'yolov4.weights')\n",
    "\n",
    "yolo = YOLOv4()\n",
    "yolo.classes = class_file\n",
    "yolo.make_model()\n",
    "yolo.load_weights(weight_file, weights_type='yolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorbike', 4: 'aeroplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'sofa', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tvmonitor', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "class_labels = {}\n",
    "with open(class_file) as f:\n",
    "    for line in f:\n",
    "        class_labels[i] = line.rstrip()\n",
    "        i += 1\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inference takes a little bit over 100 ms for a 1242 x 375 image on an Nvidia RTX 2080 Super\n",
    "\n",
    "# test_img_dir = os.path.join('kitti_single', 'testing', 'image_2')\n",
    "\n",
    "# yolo.inference(os.path.join(test_img_dir, '001002.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method predict in module yolov4.tf:\n",
      "\n",
      "predict(frame:numpy.ndarray) method of yolov4.tf.YOLOv4 instance\n",
      "    Predict one frame\n",
      "    \n",
      "    @param frame: Dim(height, width, channels)\n",
      "    \n",
      "    @return pred_bboxes == Dim(-1, (x, y, w, h, class_id, probability))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(yolo.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        loaded_img = cv2.imread(os.path.join(folder,filename))\n",
    "        images.append(loaded_img)\n",
    "    return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "def draw_bounding_boxes(img, pred_results):\n",
    "    img_height = img.shape[0]\n",
    "    img_width = img.shape[1]\n",
    "    for (i, result) in enumerate(pred_results):\n",
    "        if result[5] > threshold:\n",
    "            center_x = result[0] * img_width\n",
    "            center_y = result[1] * img_height\n",
    "            w = result[2] * img_width\n",
    "            h = result[3] * img_height\n",
    "            ul = (int(center_x - w/2), int(center_y - h/2))\n",
    "            lr = (int(center_x + w/2), int(center_y + h/2))\n",
    "            cv2.rectangle(img, ul, lr, (0, 255, 0), 2)\n",
    "            label_position = (int(center_x), int(center_y - h/2 - 10))\n",
    "            cv2.putText(img, class_labels[int(result[4])], label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_dir = os.path.join('kitti_single', 'testing', 'image_2')\n",
    "# How long (in ms) to display image after it is processed\n",
    "display_interval = 1\n",
    "\n",
    "img_list = read_images_from_folder(test_img_dir)\n",
    "for img in img_list:\n",
    "    results = yolo.predict(img)\n",
    "    img = draw_bounding_boxes(img, results)\n",
    "    cv2.imshow(\"Object Detection YOLOv4\", img)\n",
    "    cv2.waitKey(display_interval)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(250, 250)):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        loaded_img = load_img(os.path.join(folder,filename), target_size=image_size)\n",
    "        numpy_img = img_to_array(loaded_img)\n",
    "        images.append((os.path.splitext(filename)[0], numpy_img))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(mask_path, img_shape):\n",
    "    mask = np.zeros(shape=(img_shape[0], img_shape[1], 1))\n",
    "    \n",
    "    with open(mask_path) as f:\n",
    "        lines = f.readlines()\n",
    "    content = [line.split() for line in lines]\n",
    "    for item in content:\n",
    "        if item[0] == 'Car' or item[0] == 'Truck' or item[0] == 'Van' or item[0] == 'Pedestrian':\n",
    "            ul_col, ul_row = int(float(item[4])), int(float(item[5]))\n",
    "            lr_col, lr_row = int(float(item[6])), int(float(item[7]))\n",
    "            mask[ul_row:lr_row, ul_col:lr_col, 0] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_batch_generator(input_list, batch_size, resize_shape):\n",
    "    while True:\n",
    "        for img_path in input_list:\n",
    "            img_data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            img_data = cv2.resize(img_data, (resize_shape[1], resize_shape[0]))\n",
    "            img_batch.append(img_data)\n",
    "            \n",
    "            if len(img_batch) >= batch_size:\n",
    "                yield np.float32(np.stack((img_batch), 0) / 255.0)\n",
    "                img_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_mask_batch_generator(input_df, batch_size, resize_shape):\n",
    "    img_batch = []\n",
    "    mask_batch = []\n",
    "    input_list = input_df.values.tolist()\n",
    "    while True:\n",
    "        for img_path, mask_path in input_list:\n",
    "            img_data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            mask_data = create_mask(mask_path, img_data.shape)\n",
    "            img_data = cv2.resize(img_data, (resize_shape[1], resize_shape[0]))\n",
    "            mask_data = cv2.resize(mask_data, (resize_shape[1], resize_shape[0]))\n",
    "            img_batch.append(img_data)\n",
    "            mask_batch.append(mask_data)\n",
    "            \n",
    "            if len(mask_batch) >= batch_size:\n",
    "                yield np.float32(np.stack((img_batch), 0) / 255.0), np.stack(np.float32(np.expand_dims(mask_batch, -1)), 0)\n",
    "                img_batch = []\n",
    "                mask_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_dir = os.path.join('kitti_single', 'training', 'image_2')\n",
    "# train_label_dir = os.path.join('kitti_single', 'training', 'label_2')\n",
    "\n",
    "# images = [os.path.join(train_img_dir, filename) for filename in os.listdir(train_img_dir)]\n",
    "# masks = [os.path.join(train_label_dir, filename) for filename in os.listdir(train_label_dir)]\n",
    "# train_file_info_df = pd.DataFrame(np.column_stack([images, masks]), columns=['images', 'masks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_info_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_SHAPE = (224, 768)\n",
    "# BATCH_SIZE = 32\n",
    "# NUM_TEST_BATCHES = 3\n",
    "# UPSAMPLE_MODE = 'simple'\n",
    "# NET_SCALING = None\n",
    "\n",
    "# train_gen = image_mask_batch_generator(train_file_info_df, BATCH_SIZE, IMG_SHAPE)\n",
    "\n",
    "# img_batch, mask_batch = next(train_gen)\n",
    "# for i in range(3):\n",
    "#     im = np.array(255*img_batch[i],dtype=np.uint8)\n",
    "#     im_mask = np.array(mask_batch[i],dtype=np.uint8)\n",
    "    \n",
    "#     plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "#     plt.figure()\n",
    "#     plt.imshow(im_mask[:,:,0], cmap= 'gray')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras import backend as K\n",
    "\n",
    "# def calc_IOU(y_true, y_pred, smooth=1):\n",
    "#     y_true_f = K.flatten(y_true)\n",
    "#     y_pred_f = K.flatten(y_pred)\n",
    "    \n",
    "#     intersection = K.sum(y_true_f*y_pred_f)\n",
    "    \n",
    "#     return (2*(intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "# def calc_IOU_loss(y_true, y_pred):\n",
    "#     return -calc_IOU(y_true, y_pred)\n",
    "\n",
    "# def upsample_conv(filters, kernel_size, strides, padding):\n",
    "#     return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
    "\n",
    "# def upsample_simple(filters, kernel_size, strides, padding):\n",
    "#     return layers.UpSampling2D(strides)\n",
    "\n",
    "# if UPSAMPLE_MODE=='deconv':\n",
    "#     upsample=upsample_conv\n",
    "# else:\n",
    "#     upsample=upsample_simple\n",
    "\n",
    "# def create_unet():\n",
    "    \n",
    "#     input_img = layers.Input((IMG_SHAPE[0], IMG_SHAPE[1], 3), name = 'RGB_Input')\n",
    "#     pp_in_layer = input_img\n",
    "             \n",
    "#     c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(pp_in_layer)\n",
    "#     c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(c1)\n",
    "#     p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "#     c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(p1)\n",
    "#     c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(c2)\n",
    "#     p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "#     c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(p2)\n",
    "#     c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c3)\n",
    "#     p3 = layers.MaxPooling2D((2, 2)) (c3)\n",
    "    \n",
    "#     c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p3)\n",
    "#     c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "#     p4 = layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    \n",
    "#     c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p4)\n",
    "#     c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c5)\n",
    "    \n",
    "    \n",
    "#     u6 = upsample(64, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "#     u6 = layers.concatenate([u6, c4])\n",
    "#     c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u6)\n",
    "#     c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c6)\n",
    "    \n",
    "#     u7 = upsample(32, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "#     u7 = layers.concatenate([u7, c3])\n",
    "#     c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u7)\n",
    "#     c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c7)\n",
    "    \n",
    "#     u8 = upsample(16, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "#     u8 = layers.concatenate([u8, c2])\n",
    "#     c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(u8)\n",
    "#     c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(c8)\n",
    "    \n",
    "#     u9 = upsample(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "#     u9 = layers.concatenate([u9, c1], axis=3)\n",
    "#     c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(u9)\n",
    "#     c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(c9)\n",
    "    \n",
    "#     d = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "#     if NET_SCALING is not None:\n",
    "#         d = layers.UpSampling2D(NET_SCALING)(d)\n",
    "    \n",
    "#     seg_model = models.Model(inputs=[input_img], outputs=[d])\n",
    "#     seg_model.summary()\n",
    "    \n",
    "#     return seg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = Adam(learning_rate=0.0001)\n",
    "\n",
    "# model = create_unet()\n",
    "# model.compile(optimizer=adam, loss=calc_IOU_loss, metrics=[calc_IOU])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_df, val_df = train_test_split(train_file_info_df, test_size=0.25, random_state=42)\n",
    "# train_gen = image_mask_batch_generator(train_df, BATCH_SIZE, IMG_SHAPE)\n",
    "# val_gen = image_mask_batch_generator(val_df, BATCH_SIZE, IMG_SHAPE)\n",
    "\n",
    "# val_img, val_mask = [], []\n",
    "# for i in range(NUM_TEST_BATCHES):\n",
    "#     temp_img, temp_mask = next(val_gen)\n",
    "#     val_img.extend(temp_img)\n",
    "#     val_mask.extend(temp_mask)\n",
    "# val_img = np.array(val_img)\n",
    "# val_mask = np.array(val_mask)\n",
    "\n",
    "# loss_history = [model.fit(train_gen, steps_per_epoch=20, validation_data=(val_img, val_mask), epochs=120, workers=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img, test_mask = next(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_all = model.predict(test_img)\n",
    "\n",
    "# for i in range(10,20):\n",
    "\n",
    "#     im = np.array(255*test_img[i],dtype=np.uint8)\n",
    "#     im_mask = np.array(255*test_mask[i],dtype=np.uint8)\n",
    "#     im_pred = np.array(255*pred_all[i],dtype=np.uint8)\n",
    "    \n",
    "#     rgb_mask_pred = cv2.cvtColor(im_pred,cv2.COLOR_GRAY2RGB)\n",
    "#     rgb_mask_pred[:,:,1:3] = 0*rgb_mask_pred[:,:,1:3]\n",
    "#     rgb_mask_true = cv2.cvtColor(im_mask,cv2.COLOR_GRAY2RGB)\n",
    "#     rgb_mask_true[:,:,0] = 0*rgb_mask_true[:,:,0]\n",
    "#     rgb_mask_true[:,:,2] = 0*rgb_mask_true[:,:,2]\n",
    "\n",
    "#     img_pred = cv2.addWeighted(rgb_mask_pred,0.5,im,0.5,0)\n",
    "#     img_true = cv2.addWeighted(rgb_mask_true,0.5,im,0.5,0)\n",
    "\n",
    "#     plt.figure(figsize=(12,6))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(im)\n",
    "#     plt.title('Original image')\n",
    "#     plt.axis('off')\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(img_pred)\n",
    "#     plt.title('Predicted mask')\n",
    "#     plt.axis('off')\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(img_true)\n",
    "#     plt.title('Ground truth mask')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img_dir = os.path.join('kitti_single', 'testing', 'image_2')\n",
    "# images = [os.path.join(test_img_dir, filename) for filename in os.listdir(test_img_dir)]\n",
    "\n",
    "# test_gen = image_batch_generator(images, BATCH_SIZE, IMG_SHAPE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('capstone': conda)",
   "language": "python",
   "name": "python361064bitcapstonecondaccd97f80eb1d4d9b9a9bc41a0404f1e3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
